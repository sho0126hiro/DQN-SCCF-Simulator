
## シミュレータを使用した実験の検討

### 1. 実験概要

2019年度実施したSCCFの特性調査実験では、カテゴリ数が状態行動数の削減のために6個であることや、2回前までの発話内容を格納するなど、状態行動数が少ない状態での学習ができることを確認した。

本実験では、状態行動数を増やし、深層強化学習アルゴリズムをSCCFに適用することで学習が可能であるかについて、性能を調査する。

ただし、実験を簡単にすることで、サイクルを早くするため、IDAは実際の利用者ではなく、エージェントを評価するシミュレータを対象として実験を実施する。

#### 対象アルゴリズム

- REINFORCE
- DQN（DQNの派生系も実装する予定です）
- そのほか未定

### 2. エージェントのパラメータ/実験環境

- 状態: 過去T回までの発話したカテゴリ
- 行動: 次に発話するカテゴリ
- エージェントが発話するカテゴリの総数: C
- エージェントが持つ、過去の発話履歴の長さ: T (Tステップ前までの発話を保存)

よって、エージェントのもつ状態行動の総数は 
$$
状態 \cdot 行動 = C^T \cdot C = C^{T+1}
$$

- 実験では、次の状態行動パターンで学習を行うことを検討している

  - C = 6, T = 2   状態行動数: 6^3 = 216 (2019年度の実験設定)
  - C = 10, T = 2  状態行動数: 10^3 = 1000
  - C = 6, T = 4 状態行動数: 6^5 = 7776
  - C = 15, T = 3 状態行動数: 15^4 = 50625
  - C = 20, T = 3 状態行動数: 20^4 = 16,0000
  - C = 15, T = 4 状態行動数: 15^5 = 75,9375
  - C = 20, T = 4 状態行動数: 20^5 = 320,0000
  - C.= 25, T = 3 状態行動数: 25^4 = 39,0625
  - C = 50, T = 

- 参考: エージェントの学習アルゴリズムにDQNなどを使用する場合のNNの入力と出力

  入力: 状態（過去Tステップまでに発話したカテゴリ番号）, 行動

  出力:  行動価値

  入力ノード数: T+1, 出力ノード数1

  そのため、Tを増加させるとノード数が増えるが、Cはいくら増やしてもノード数に変化がない。

### 3. シミュレータのパラメータ設定

- シミュレータがエージェントが与える報酬: 

$$
r \in \{1, 0, -1\}
$$

​	DQNの場合、報酬値をClipping(1,0,-1の値に絞る)することで学習が安定化するため。

​	2019年度のREINFORCEの実験で性能の良かった実験設定（報酬設定B）

- シミュレータがエージェントに与える報酬がrである確率

$$
P(r|s,n): 特定の発話c\in Cがn回連続で続いた時、さらにcが発話された条件のもと、報酬rを与える確率
\\
ただし、
\\
\\
R_1 =  P(r=1|s = s^*, n=n^*)
\\
R_0 = P(r=0|s = s^*, n=n^*)
\\
R_{-1} = P(r=-1|s = s^*, n=n^*)
\\
とおいたうえで、R_1 + R_0 + R_{-1} \neq 1 の時、ルーレット選択を行う。
すなわち
\\
R_r = \frac{R_r}{R_1 + R_0 + R_{-1}} (r \in \{1, 0, -1\})
$$

 

具体例:
$$
P_{x}(r=1|c,n) =[
\\ ここに示す、配列の1番目から、n=1, n=2, n=3, ...
\\
\ \ \ \ [0.5, 0.9, 0.9, ... ] (2回目以降 α=1.0) (c=1) \\ 
\ \ \ \ [0.4, 0.45, 0.1, ...] (3回目以降: α=0.22) (c=2)\\
\ \ \ \ [0.5, 0.15, ...] (3回目以降: α=0.3) (c=3)\\
\ \ \ \ [0.35, 0.5, ...] (3回目以降: α=0.3) (c=4)\\
\ \ \ \ [0.45, ... ](3回目以降: α=0.1) (c=5)\\
\ \ \ \ [0.4, ... ](3回目以降: α=0.1) (c=6)\\
]
$$

$$
ただし、ここで\alpha: 減衰係数\\
（上記において...と示している箇所は、その一つ前の数に減衰係数をかけた値として続く）\\
例: 
P_{x}(r=1|c=2, n) = [0.4, 0.45, 0.1, 0.1\cdot0.2, 0.1\cdot0.2^2, 0.1\cdot0.2^3, ...]
$$

また、減衰係数がかかる前までの、値が記載されている箇所は、2019年度の実験で、実際に実験協力者が与えた報酬確率を用いている（実測値を有効数字2桁で四捨五入）。ただし確率が0の箇所については無視している。

減衰係数の決め方
$$
減衰係数がかかるまでに2つデータがある箇所: 直前の勾配を利用\\
例: P_{x}(r=1|c=1,n) = [0.4.0.45,0.1, ...]\ \ この時\ a=0.1/0.45=0.22
\\
ただし、直前の勾配が1より大きい時: \alpha=0.3
\\
直前の勾配が1の時: \alpha=0.7
\\
直前のデータがない箇所: \alpha=0.3
$$


### 4. 実際に利用するシミュレータのパラメータ

以上の設定の元、SCCF特性調査実験において、特徴的だった実験協力者4名のデータから、パラメータを用意する。

**これらのパラメータはC=6,T=2(2019年度の実験と同じ状態行動設定)の場合でのみ使用する。**

- user1

元データ:

```
P(r|c,n) =
[ (r = 1)
		
  // [確定値] (その後の値: 減衰係数をかけていく)
	// 確定値の配列番号は、同じ発話を繰り返した回数を示す。
    [0.52, 0.91, 0.92, 0.9] (α = 0.98) (カテゴリ番号:1)
    [0.4	, 0.44] (α = 0.3) (カテゴリ番号:2)
    [0.48, 0.13] (α = 0.27) (カテゴリ番号:3)	
    [0.35,	0.5]	(α = 0.3) (カテゴリ番号:4)
    [0.45] (α = 0.3) (カテゴリ番号:5)
    [0.38] (α = 0.3) (カテゴリ番号:6)
],
[ (r = 0)
		[0.45, 0.09, 0.08] (α = 0.89)
    [0.55, 0.33] (α = 0.3)
    [0.5, 0.88]	(α = 0.3)
    [0.55, 0.5]	(α = 0.91)
    [0.4, 1, 1] (α = 0.7)
    [0.5, 1, 1] (α = 0.7)
],
[ (r = -1)
	[0.01] (α = 0.3)
  [0.05, 0.2] (α = 0.7)	
  [0.02] (α = 0.3)	
  [0.1] (α = 0.3)	
  [0.15] (α = 0.3)
  [0.1] (α = 0.3)
]
```
- user4
  Chatbot-REINFORCEでの実験結果: 1,3位のカテゴリの発話確率が増加

  ```
  調査中
  ```

  

- user7
  Chatbot-REINFORCEでの実験結果: 2,4位のカテゴリの発話確率が増加

  ```
  調査中
  ```


- user8
  Chatbot-REINFORCEでの実験結果: 3,4位のカテゴリの発話確率が増加

  ```
  調査中
  ```

  
