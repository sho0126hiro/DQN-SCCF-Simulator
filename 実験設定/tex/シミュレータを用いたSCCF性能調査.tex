\documentclass[11pt,a4paper]{jsarticle}
%
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{ascmac}
\usepackage{color}
\usepackage{listings,jlisting} %日本語のコメントアウトをする場合jlistingが必要
%ここからソースコードの表示に関する設定
\lstset{
  basicstyle={\ttfamily},
  identifierstyle={\small},
  commentstyle={\smallitshape},
  keywordstyle={\small\bfseries},
  ndkeywordstyle={\small},
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},
  xrightmargin=0zw,
  xleftmargin=3zw,
  numberstyle={\scriptsize},
  stepnumber=1,
  numbersep=1zw,
  lineskip=-0.5ex
}

%
\setlength{\textwidth}{\fullwidth}
\setlength{\textheight}{39\baselineskip}
\addtolength{\textheight}{\topskip}
\setlength{\voffset}{-0.5in}
\setlength{\headsep}{0.3in}
%
\newcommand{\divergence}{\mathrm{div}\,}  %ダイバージェンス
\newcommand{\grad}{\mathrm{grad}\,}  %グラディエント
\newcommand{\rot}{\mathrm{rot}\,}  %ローテーション
%
\pagestyle{myheadings}
\markright{\footnotesize \sf 2020/07 1AS 廣瀬翔 \ SCCF性能調査-実験設定}
\begin{document}
%
%
\section*{シミュレータを用いたSCCFの性能調査　実験設定}
\section{実験概要}

2019年度実施したSCCFの特性調査実験では、カテゴリ数が状態行動数の削減のために6個であることや、2回前までの発話内容を格納するなど、状態行動数が少ない状態での学習ができることを確認した。

本実験では、深層強化学習アルゴリズムをSCCFに適用することで学習が可能であるかについて、性能を調査する。
まず、昨年度の実験と等しい数の状態行動数で深層強化学習アルゴリズムを適用し、性能を評価する。
加えて、状態行動数を増加させ、性能を評価する。
ただし、実験を簡単にすることで、サイクルを早くするため、IDAは実際の利用者ではなく、エージェントを評価する
人間の評価の特徴を模倣したシミュレータを対象として実験を実施する。

○対象アルゴリズム
\vspace{-0.5\baselineskip}{           %3. 上の余白
\begin{itemize}
    \item REINFORCE
    \item Deep-Q-Network (DQN)
    \item Double DQN
    \item Dueling Network
    \item PPO(Proximal Policy Optimization Algorithms)など
    \item その他、時間があれば以下の環境で色々試したいです。
\end{itemize}
}

\section{エージェントのパラメータ設定}
エージェントのもつパラメータは以下の通り。

\vspace{-0.5\baselineskip}{           %3. 上の余白
\begin{itemize}
    \item 状態$s\in \bm{S} $: 過去T回までに発話したカテゴリ（順序付き）
    \item 行動$a\in \bm{A} $: 次に発話するカテゴリ（今から発話するカテゴリ） => 特徴ベクトル
    \item エージェントが発話するカテゴリの種類: $c = |\bm{C}|, \bm{C}: エージェントの発話するカテゴリの集合$
    \item エージェントが持つ、過去の発話履歴の長さ: T  (Tステップ前までの発話を保存)

\end{itemize}
}

以上より、エージェントのもつ状態行動の総数は,
$$
|\bm{S}| \cdot |\bm{A}| = c^t \cdot c = c^{t+1}
$$
\newpage
\section{実験設定}
実験では、エージェントのパラメータを次のように変化させ、シミュレータを対象として学習の性能を確認する。

\vspace{-0.5\baselineskip}{           %3. 上の余白
\begin{itemize}
    \item $C = 6,\ T=2,\ |\mbox{\bf S}| \cdot |\mbox{\bf A}| = 6^3 = 216$
    \item $C = 10,\  T=2,\ |\mbox{\bf S}| \cdot |\mbox{\bf A}| = 10^3 = 1,000$
    \item $C = 15, \ T=3,\ |\mbox{\bf S}| \cdot |\mbox{\bf A}| = 15^4 = 50,625$
    \item $C = 20, \ T=3,\ |\mbox{\bf S}| \cdot |\mbox{\bf A}| = 20^4 = 160,000$
    \item $C = 50, \ T=3,\ |\mbox{\bf S}| \cdot |\mbox{\bf A}| = 50^4 = 6,250,000$
    \item $C = 75, \ T=3,\ |\mbox{\bf S}| \cdot |\mbox{\bf A}| = 75^4 = 31,640,625$
    \item $C = 50, \ T=4,\ |\mbox{\bf S}| \cdot |\mbox{\bf A}| = 50^5 = 312,500,000$
    \item $C = 100, \ T=3,\ |\mbox{\bf S}| \cdot |\mbox{\bf A}| = 100^4 = 100,000,000$
\end{itemize}
}

参考：エージェントの学習アルゴリズムにDQNなどを使用する場合のNNの入力と出力
\vspace{-0.5\baselineskip}{           %3. 上の余白
\begin{itemize}
    \item 入力: 状態+行動　（入力ノード数： $ T + 1 $）
    \item 出力: 行動価値
\end{itemize}
}
よって、Tの大きさは入力ノードの数と直結する一方、Cの大きさは、入力ノード数とは関係ない。

\section{シミュレータの設定}
シミュレータは、エージェントの振舞を評価する。（エージェントに報酬を与える）

\subsection{シミュレータの与える報酬}

2019年度のREINFORCEの実験で性能がよかった実験設定（報酬設定B）を採用する。
報酬は、{良い,興味なし, 悪い}に対応している。

$$
r \in \{1, 0, -1\}
$$

\subsection{シミュレータがエージェントに与える報酬について}

シミュレータがカテゴリ番号cの内容を、t回連続して発話した条件の元、エージェントに与える報酬がrである確率を$P(r|c,t)$と定義する。

ただし、c*t*は任意のc

-> 確率変数を大文字にするべき（Rを大文字に）
直近のあたいが

cとtに定義域をつける必要あり

$$
R_1 =  P(r=1|c = c^*, t=t^*)
$$
$$
R_0 = P(r=0|c = c^*, t=t^*)
$$
$$
R_{-1} = P(r=-1|c = c^*, t=t^*)
$$
とおいたうえで、$R_1 + R_0 + R_{-1} \neq 1$の時、ルーレット選択を行う。

ルーレット選択の部分はいらない（実装上のまるめごさのけんは、プログラムないで勝手にやればいい。）

すなわち
$$
R_r = \frac{R_r}{R_1 + R_0 + R_{-1}} (r \in \{1, 0, -1\})
$$

\subsection{報酬付与確率の具体例}
報酬付与確率の具体例を下記に示す。
\begin{lstlisting}
P_ex(r|c,t) =
[ (r = 1)
    // 配列に示された値は固定値である。
    // 固定値の配列番号は、同じ発話を繰り返した回数を示す。
    ([t=1の時r=1を与える確率, t=2の時, t=3, t=4])
    [0.52, 0.91, 0.92, 0.9] (α = 0.98) (カテゴリ番号c:1)
    [0.4 , 0.44] (α = 0.3) (カテゴリ番号c:2)
    [0.48, 0.13] (α = 0.27) (カテゴリ番号c:3)	
    [0.35,	0.5]	(α = 0.3) (カテゴリ番号c:4)
    [0.45] (α = 0.3) (カテゴリ番号c:5)
    [0.38] (α = 0.3) (カテゴリ番号c:6)
],
\end{lstlisting}

ただし、ここで$\alpha$は減衰係数であり、固定値で定められない範囲の報酬確率を決める定数である。\par
ex.: 
$$
P_{ex}(r=1|c=3, n) = [0.48, 0.13, 0.13\cdot0.27, 0.13\cdot0.27^2, 0.13\cdot0.27^3...]
$$

減衰係数の決め方:

- 減衰係数がかかるまでに2つデータがある箇所: 直前の勾配を利用\par
ex.
$$
P_{x}(r=1|c=1,t) = [0.4.0.45,0.1, ...]\ \ この時\ \alpha=0.1/0.45=0.22
$$
\par
- ただし、直前の勾配が1より大きい時: $\alpha=0.3$ \par
- 直前の勾配が1の時: $\alpha=0.7$  \par
- 直前のデータがない箇所: $\alpha=0.3$  \par

\newpage

\section{実験に利用するシミュレータのパラメータ}

以上の設定の元、2019年度のSCCF特性調査実験において、特徴的だった実験協力者4名のデータを用いてパラメータ（報酬付与確率）を用意した。
下記に示す報酬付与確率の設定は、$C=6$の時のみ適用できる。\par
$C=20$など、カテゴリ数を増加させる場合のカテゴリごとの報酬付与確率は、6つパラメータと全く同じものをランダムにコピーして、合計20個になるように指定する。\par
\par

具体例: user1\par
- 1位から6位まで順に発話確率が高かった。（2019年度の実験結果）\par
- 報酬の与え方の特徴: 報酬1or0を与える確率が高かった。\par
\begin{lstlisting}
P(r|c,t) =
[ (r = 1)
    // [確定値] (その後の値: 減衰係数をかけていく)
    // 確定値の配列番号は、同じ発話を繰り返した回数を示す。
    [0.52, 0.91, 0.92, 0.9] (α = 0.98) (カテゴリ番号:1)
    [0.4	, 0.44] (α = 0.3) (カテゴリ番号:2)
    [0.48, 0.13] (α = 0.27) (カテゴリ番号:3)	
    [0.35,	0.5]	(α = 0.3) (カテゴリ番号:4)
    [0.45] (α = 0.3) (カテゴリ番号:5)
    [0.38] (α = 0.3) (カテゴリ番号:6)
],
[ (r = 0)
    [0.45, 0.09, 0.08] (α = 0.89)
    [0.55, 0.33] (α = 0.3)
    [0.5, 0.88]	(α = 0.3)
    [0.55, 0.5]	(α = 0.91)
    [0.4, 1, 1] (α = 0.7)
    [0.5, 1, 1] (α = 0.7)
],
[ (r = -1)
    [0.01] (α = 0.3)
    [0.05, 0.2] (α = 0.7)	
    [0.02] (α = 0.3)	
    [0.1] (α = 0.3)	
    [0.15] (α = 0.3)
    [0.1] (α = 0.3)
]
\end{lstlisting}
\end{document}